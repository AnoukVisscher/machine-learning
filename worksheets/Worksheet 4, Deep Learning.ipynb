{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work in progress, use at your own risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: This worksheet assumes that you've watched the first Deep Learning lecture. If you do the worksheet before that, you may have to google some of the phrases.**\n",
    "\n",
    "For this worksheet, we need to install Keras. Execute the cell below, or run the command without the exclamation point in a terminal, command prompt or anaconda prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 4: Deep Learning\n",
    "\n",
    "## Keras\n",
    "\n",
    "Keras is a frontend for other deep learning libraries (Tensorflow, Theano, CNTK), which implements most basic nerual network architectures in a simple framework. The default backend is Tensorflow, which should be installed automatically with the pip command above (if not, run ```pip install tensorflow```).\n",
    "\n",
    "Deep learning models are trained by gradient descent. These models are often so complex, that we don't want to have to work out the gradient ourselves. Keras and tensorflow allow the gradient to be computed automatically: we write down our model as a _computation graph_, compile it, and the system works out the gradients for us. This also makes it easier to run things on a GPU (if you have one available, this worksheet doesn't require a GPU).\n",
    "\n",
    "Before qwe start building models, let's familiarize ourselves with this idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The first time you run this, tensorflow is started up, which may take a while.\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make two 2x2 matrices and sum them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = K.placeholder(shape=(2,2))\n",
    "y = K.placeholder(shape=(2,2))\n",
    "\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you may be wondering what's going on here. x and y have a shape, but we haven't defined what values are in the matrix. How can we sum them before we have put values in them?\n",
    "\n",
    "The trick here is that x and y are'nt matrices like in numpy, they're placeholders for matrices. ```z``` is actually an _object_ that stores references to objects ```x``` and ```y```. In other words, we have a _computation graph_ with nodes x and y. \n",
    "\n",
    "Here's what z looks like under water:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node {\n",
       "  name: \"Placeholder\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "        dim {\n",
       "          size: 2\n",
       "        }\n",
       "        dim {\n",
       "          size: 2\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Placeholder_1\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "        dim {\n",
       "          size: 2\n",
       "        }\n",
       "        dim {\n",
       "          size: 2\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add\"\n",
       "  op: \"Add\"\n",
       "  input: \"Placeholder\"\n",
       "  input: \"Placeholder_1\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "versions {\n",
       "  producer: 21\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to understand what that means, just that it says that the graph contains three nodes (x, y and z) and that z if derived from x and y by summing.\n",
    "\n",
    "We can now compile this graph, put some values at nodes x and y, compute it and retrieve the output z. Since Keras isn't meant to be used at this level we'll show you briefly how it works in tensorflow (usually we'll let Keras handle all this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.77617455,  1.01602781],\n",
       "       [ 1.50804496,  1.79550886]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_value = np.random.rand(2,2)\n",
    "    y_value = np.random.rand(2,2)\n",
    "    \n",
    "    result = session.run(z, feed_dict={x: x_value, y: y_value})\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes the computation graph we've just made and tells us that we want to use if as a function with inputs ```x``` and ```y``` and we want to compute the value of ```z```. \n",
    "\n",
    "If you want, you can think of x and y as the \"input nodes\" of a neural network, and z as the \"output node\".\n",
    "\n",
    "We can also make a _Variable_. A variable is also a node in the computation graph, but one that actually contains a value that is retained even if the inputs are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = K.placeholder(shape=(2,2))\n",
    "b = K.variable(value=np.random.rand(2,2)) # not that we have to provide an actual matrix\n",
    "\n",
    "c = K.dot(a, b) # matrix multiplication\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c is still a node in the computation graph, and again, we can compile and run the graph for some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.47241163,  0.18284869],\n",
       "       [ 1.19554937,  0.13533755]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer()) # low level stuff, normally Keras handles this\n",
    "\n",
    "    a_value = np.random.rand(2,2)\n",
    "    \n",
    "    result = session.run(c, feed_dict={a: a_value})\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these principles, we can build neural networks. We use placeholders for the input, outputs and hidden layers, and variables for the weights. The variables persist between inputs and get changed during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with a simple neural network model\n",
    "\n",
    "Now, let's build a simple neural network. We'll start by loading the MNIST data that we saw in the first lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape) \n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set is one 3-tensor of 60000 images of 28x28 pixels. The test set contains 10000 additional images.\n",
    "\n",
    "Note that the data is already split in a canonical train and test set.\n",
    "\n",
    "For this network, we don't care about the structure of the image, so we'll flatten everything into a 784-dimensional feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, -1)\n",
    "x_test = x_test.reshape(10000, -1)\n",
    "\n",
    "print(x_train.shape) \n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we'll use is a simple fully connected feedforward network. This is called a Dense layer in Keras. Since fully connected layers are a bit heavy on image data (and you're running this on your laptops), we'll reduce the dimensionality of the data by PCA (see the Methodology 2 lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 60)\n",
      "(10000, 60)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=60) # reduce to 60 dimensions\n",
    "pca.fit(x_train)\n",
    "\n",
    "x_train = pca.transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "print(x_train.shape) \n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training labels are encoded as integers. We need these as one-hot vectors instead, so we can match them to the ten outputs of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (10000,)\n",
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are now encoded as vectors of length 10 with zeros everywhere, except at the true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create a model. Keras has two APIs for this: the Sequential API and the Model API. The sequential API (the simplest) assumes that your model is a simple sequence of operations, usually neural network layers. The inputs is passed through the first layer, the result of that is passed through the second and so on. \n",
    "\n",
    "This is useful for simple NN models where you are only interested in the input and output. If your model hgets more complex, you may want to uyse the Model API (we'll discuss that below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               7808      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 9,098\n",
      "Trainable params: 9,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(60,))) # first dense layer, 32 hidden units\n",
    "model.add(Activation('relu'))           # activation layer\n",
    "model.add(Dense(10))                    # second dense layer\n",
    "model.add(Activation('softmax'))        # output class probabilities\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note:\n",
    "\n",
    "* For the first layer we need to provide the input shape. For the second, this is not necessary, because Keras can _infer_ the input shape from the layers before it.\n",
    "* The Dense layers are just linear operations (multiply by a weight matrix, add a bias vector). The activation functions are added as separate layers (activation_1, and activation_2). You can also pass the activation as an argument to the Dense layer.\n",
    "* Keras picks a sensible default weight initialization for us, and applies it (this model already has initial weights).\n",
    "* The last layer has 10 nodes (one for each class) with a softmax activation. This means we can interpret the output as class probabilities.\n",
    "* Even though we specified a one-dimensional input, the model summary shows two-dimensional shapes with the first dimension always ```None```. This is the _batch dimension_. Neural networks are almost never trained/run one input at a time; we usually feed them several inputs together (a mini-batch). Kears assumes this is the case. So, if we choose a batch size of ten, our input dimension would become (10, 60 ) and our first hidden layer would have dimensions (10, 32). Keras can be flexible about the batch size so we don't have to specify it now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a complete computation graph, we need more than just a model: we need a loss function as well. We also need to specify which optimizer we're going to use. \n",
    "\n",
    "Let's use categorical cross-entropy as loss, together with the _Adam_ optimizer (all these optimizers are fancy variations on gradient descent).\n",
    "\n",
    "With this information, we can compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also told the compiler that we'd lie it to compute accuracy for us during training (since categorical cross-entropy is a bit hard to interpret).\n",
    "\n",
    "We're now ready to start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s - loss: 7.3782 - acc: 0.5393     \n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s - loss: 6.0021 - acc: 0.6258     \n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s - loss: 5.5733 - acc: 0.6528     \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s - loss: 5.4838 - acc: 0.6587     \n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s - loss: 5.4009 - acc: 0.6638     \n"
     ]
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my laptop (no GPU), this takes about 5 seconds per epoch (an epoch is a run of the whole data).\n",
    "\n",
    "Note that these losses/accuracies are on the training data. Of course, **we don't want to use the test data at this stage to see how well we're doing**. We can tell Keras to withhold some validation data, so we can get an indication of the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 5s - loss: 5.4147 - acc: 0.6630 - val_loss: 5.3073 - val_acc: 0.6700\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 5s - loss: 5.3155 - acc: 0.6696 - val_loss: 5.2881 - val_acc: 0.6712\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 5s - loss: 5.3568 - acc: 0.6668 - val_loss: 5.3289 - val_acc: 0.6683\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 5s - loss: 5.3388 - acc: 0.6682 - val_loss: 5.2002 - val_acc: 0.6767\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 5s - loss: 5.3396 - acc: 0.6680 - val_loss: 5.2177 - val_acc: 0.6758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1408ecf60>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model remembers its weights so we've now trained it for 10 epochs, not 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
